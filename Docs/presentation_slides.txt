CISPA European AI &
Cybersecurity Championship
Adam Dziedzic and Franziska Boenisch
nd
Introduction on November 22 , 2025

SprintML

Who are we? Franziska & Adam
Google, Microsoft
Barclays, DTU,
EPFL, CERN, WUT
Universities of
Chicago & Toronto,
Vector, CISPA

University of Toronto,
Vector, CISPA, FU Berlin
AbiBac: Berlin & Lyon,
Fraunhofer AISEC,
TU Eindhoven, Chung Cheng
University in Taiwan
2

The Technical CISPA Team at the Hackathon

Aditya
Kumar

Lorenzo
Rossi

Michel
Meintz

BartÅ‚omiej
Marek

Bihe
Zhao

Vincent
Hanke

Ivo
Hoese

Maitri
Shah

Nima
Dindarsafa

Louis
Kerner

Research about Security & Machine Learning
Located in SaarbrÃ¼cken (<2h from Paris/Frankfurt)
International community with > 600 employees
from 52 different nations
> 40 Faculty Members and their research groups
4

What makes us proud at CISPA?

https://csrankings.org/#/fromyear/2014/toyear/2024/index?sec&world

5

Tasks from Our SprintML Group at CISPA
S - ecure
P - rivate
R - obust
In - terpretable
T â€“ rustworthy
Machine Learning
~20 members (PhDs, Postdocs,
Research Interns, Students)
8 different nations
Visit: sprintml.com
6

European Championship
First Regional Round in Paris
November 22-23, 2025

Awards
The top 3 teams advance to the grand
finale at CISPA in July next year.
We sponsor the travel to the finale.

Prizes for the Grand Finale:
1st place: â‚¬4,000
2nd place: â‚¬2,000
3rd place: â‚¬1,200
Runner-ups: â‚¬400

8

Plan for Our 24-hour Hackathon in Paris
Saturday:
11:00 â€“ Participant registration and snacks provided
12:00 â€“ Opening plenary and information
12:30 â€“ Hackathon begins
17:00 â€“ Dinner delivered
Sunday:
08:00 â€“ Breakfast delivered
12:30 â€“ Hackathon ends & Lunch delivered
13:30 â€“ Presentation of the tasks
14:30 â€“ Assessment meeting to determine winners
15:00 â€“ Announcement of results (award ceremony)

9

Communication via Discord
Join: https://discord.gg/PDzPcaZPr8

10

Teams
Only the teams that have the label
multi were assigned an API key.
We would like the participants with
the label single to form a full team.

11

Teams: Where to go?

12

If you donâ€™t want to
be on the photos:
request a different
badge from Julian

13

Leaderboard: Ranking of All the Teams

14

Task 1: Adversarial Examples

Machine Learning (ML) models are powerfulâ€¦

Pomeranian

16

â€¦ but they are easily fooled!

Panda?
Gibbon!

Goodfellow et al., ICLRâ€™15

Panda!

17

Misclassification through targeted perturbations

=
Adversarial Example
Gibbon
97.7% Confidence
Goodfellow et al., ICLRâ€™15

+
Carefully
Crafted Noise

Original Image
Panda
57.7% Confidence
18

Your TASK1: Find Adversarial Examples
query

logits

You are the attacker! 100 benign images ğ‘¥
We provide you with a model.
Your goal: Get the model to mis-predict!
Evaluation: Get the lowest score.
Score: Ïƒ100
0 ğ‘ ğ‘– where ğ‘ ğ‘– = 1, if the model still correctly classifies the sample
and ğ‘ ğ‘– = ||ğ‘¥ ğ‘œğ‘Ÿğ‘–ğ‘” âˆ’ ğ‘¥ ğ‘¦ğ‘œğ‘¢ğ‘Ÿğ‘  ||2 normalized L2-distance otherwise
19

Task 2: Image Attribution

Generative Artificial Intelligence (GenAI)
OpenAI

stability.ai

SUNO

21

GenAI Trained on a Vast Amount of Data
DATA

Train

22

GenAI Inputs and Outputs Form Data Loops

Train

Generate

23

Data Loops: Misinformation & Degeneration

Train

Generate

Collapse

24

Identify Training Data and Generated Outputs

EU
AI Act

Train

Generate

2nd
August
2026

25

AutoRegressive Models (ARs) for Language
An example of text generation: Predict the next token based on all the previous tokens.

She ate an _______
She

ate

an

?

26

AutoRegressive Models (ARs) for Language
An example of text generation: Predict the next token based on all the previous tokens.

She ate an apple
She

ate

an

apple

ğ‘¡=ğ‘‡

ğ‘ ğ‘¥1 , ğ‘¥2 , â€¦ , ğ’™ğ‘» = à·‘ ğ‘ ğ’™ğ‘» ğ‘¥1 , ğ‘¥2 , â€¦ , ğ‘¥ğ‘‡âˆ’1 ) , ğ’™ğ‘» âˆˆ ğ‘‰
ğ‘¡=1
27

AutoRegressive Models (ARs) for Language
ğ‘¡=ğ‘‡

ğ‘ ğ‘¥1 , ğ‘¥2 , â€¦ , ğ’™ğ‘» = à·‘ ğ‘ ğ’™ğ‘» ğ‘¥1 , ğ‘¥2 , â€¦ , ğ‘¥ğ‘‡âˆ’1 ) , ğ’™ğ‘» âˆˆ ğ‘‰
ğ‘¡=1

Main advantages of autoregressive models for
language:
1. Scaling laws: the modelsâ€™ performance scales well with
increased model size, data, and computation.
2. Zero-shot generalization: the models perform well on
tasks that they were not trained on (e.g., classification tasks
without using a super-vised fine-tuning).
28

Vision Autoregressive Models (VARs)

[Tian et al. NeurIPSâ€™24]

Predict the next scale (or resolution) with many tokens per scale: coarse to fine scale.

29

VAR involves two training steps

Stage1: train a multi-scale Vector
Quantized (VQ) autoencoder.
This encodes an initial image into K
token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ , shown in
the middle of the picture above.

30

VAR involves two training steps

Stage1: train a multi-scale Vector
Quantized (VQ) autoencoder.
This encodes an initial image into K
token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ , shown in
the middle of the picture above.

Stage2: a VAR transformer is trained via
the next-scale prediction. It takes
ğ‘  , ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾âˆ’1 as input and predicts
the ğ‘Ÿğ¾ , where ğ‘  is a start token with
condition information (e.g., image class).

31

Stage 1: Train Multi-Scale VQ-VAE on Images
Stage1: train a multi-scale Vector Quantized (VQ) variational autoencoder to encode
an initial image into K token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ and then reconstruct the image.

32 x 16 x 16

3 x 256 x 256

Encode
(Conv)
Initial image

Latent
Encode
ğ‘“ â€“ feature
map

32

Stage 1: Train Multi-Scale VQ-VAE on Images
Stage1: train a multi-scale Vector Quantized (VQ) variational autoencoder to encode
an initial image into K token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ and then reconstruct the image.

32 x 16 x 16

3 x 256 x 256

Encode
(Conv)
Initial image

Latent
Encode

Tokens

ğ‘“ â€“ feature
map
32 float values

4096 entries in
the codebook
(vocabulary size)

32 float values
â€¦
32 float values

33

Stage 1: Train Multi-Scale VQ-VAE on Images
Stage1: train a multi-scale Vector Quantized (VQ) variational autoencoder to encode
an initial image into K token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ and then reconstruct the image.
32 x 16 x 16

3 x 256 x 256

Encode
(Conv)
Initial image

Latent
Encode
ğ‘“ â€“ feature
map

4096 entries in
the codebook
(vocabulary size)

Tokens

3 x 256 x 256
Latent
Decode

Decode
(Conv)

ğ‘“áˆ˜ â€“
reconstructed
feature
32 float values
map
32 float values

Output Image

â€¦
32 float values

34

VAR involves two training steps

Stage1: train a multi-scale Vector
Quantized (VQ) autoencoder.
This encodes an initial image into K
token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ , shown in
the middle of the picture above.

Stage2: a VAR transformer is trained via
the next-scale prediction. It takes
ğ‘  , ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾âˆ’1 as input and predicts
the ğ‘Ÿğ¾ , where ğ‘  is a start token with
condition information (e.g., image class).

35

Stage 2: Train VAR Transformer on Tokens
Expected:

ğ‘Ÿ1

Predicted:

ğ‘Ÿ1Æ¸

Cross-Entropy

VAR Transformer
S
Start Token
36

Stage 2: Train VAR Transformer on Tokens
Expected:

ğ‘Ÿ1

ğ‘Ÿ2 = [ğ‘¡1 , ğ‘¡2 , ğ‘¡3 , ğ‘¡4 ]

Predicted:

ğ‘Ÿ1Æ¸

ğ‘Ÿ2Æ¸ = [ğ‘¡1 , ğ‘¡2 , ğ‘¡3 , ğ‘¡4 ]

Cross-Entropy

VAR Transformer
S

ğ‘Ÿ1

Start Token
37

VAR involves two training steps

Stage1: train a multi-scale Vector
Quantized (VQ) autoencoder.
This encodes an initial image into K
token maps ğ‘… = ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ , ğ‘Ÿğ¾ , shown in
the middle of the picture above.

38

Your TASK2: Training Data Attribution

Model 1: VAR

Model 2: RAR

?
The internet is full of generated data.

Model 3: Stable Diffusion

39

Your TASK2: Training Data Attribution

Model 1: VAR

Model 2: RAR

We give you 10,500 images.
Generated by VAR, RAR, SD, Taming
and OOD (e.g. natural images)
Your goal: correctly identify the source of
each image!
The score: average over per-class FPRs and
TPRs.

Model 3: Stable Diffusion (SD)

40

Your TASK2: Training Data Attribution
TRP (True Positive Rate): Measures how effectively model
recognizes images from each model.
FPR (False Positive Rate): Measures when you incorrectly
label another image to a given model.
For each of the 4 models, we compute:
Final Score = TPR/ (1 + (W*FPR)), with weight W=5
Then, we average over all 4.
41

Task 3: Watermark Detection

Question: Is an image generated or not?
Detector

Hard to
predict!

43

Question: Is an image generated or not?

Watermark

Detector

Hard to
predict!

Detector

Generated
44

Vision Autoregressive Models (VARs)

[Tian et al. NeurIPSâ€™24]

Predict the next scale (or resolution) with many tokens per scale: coarse to fine scale.

45

Infinity: State-of-the-Art Image Generation
[Han et al. CVPRâ€™25]

ğ‘Ÿ3
ğ‘Ÿ2
ğ‘Ÿ1

46

BitMark for Bit-Wise Image Generation
ğ‘Ÿ3
ğ‘Ÿ2
ğ‘Ÿ1

1

47

Embedding BitMark during Generation
1

?

Red List R: {01,10}
Green List G: {11,00}
exp(ğ‘™1 )
ğ‘1 =
exp ğ‘™1 + exp ğ‘™0
exp(ğ‘™0 )
ğ‘0 =
exp ğ‘™0 + exp ğ‘™1
48

Embedding BitMark during Generation
1

1

Red List R: {01,10}
Green List G: {11,00}
exp(ğ‘™1 + ğœ¹)
ğ‘1 =
exp ğ‘™1 + ğœ¹ + exp ğ‘™0
exp(ğ‘™0 )
ğ‘0 =
exp ğ‘™0 + exp ğ‘™1 + ğœ¹
49

Detecting BitMark Watermarked Images
1
1

1
1

â„‹0 : The sequence of bits is generated with no knowledge about
the green and red lists.
ğ›¾ğ‘‡ green bit sequences

(1 âˆ’ ğ›¾)ğ‘‡ red bit sequences

ğ‘§ = (ğ¶ âˆ’ ğ›¾ğ‘‡)/ ğ‘‡ğ›¾(1 âˆ’ ğ›¾)

50

Your TASK3: Detect Watermarked Images
s1
s2
s3

For each image, you return a continuous
score s between 0 (certainly no watermark)
and 1 (certainly watermarked): score s âˆˆ [0,1]
score â‰¥ 0.5 â€“ watermarked image
score < 0.5 â€“ non-watermarked image (clean)

We give you images.
51

Metric for TASK3: Watermarked Images
s1
s2
s3
You provide the scores.

Metric: True Positive Rate (TRP) at 1%
False Positive Rate
â†’ The higher TPR the better
A high TPR means the detector correctly
identifies many watermarked images.
The FPR is fixed at 1%, meaning that only
1% of clean images are allowed to be
falsely scored as watermarked
(score â‰¥ 0.5).
52

Compute Infrastructure

Compute from JÃ¼lich Research Center
Compute Infrastructure:
1. 40 compute nodes (1 compute node reserved per your job).
2. Each compute node with:

a) 4Ã— NVIDIA A100 GPU, 4Ã— 40 GB HBM2e
b) 2Ã— AMD EPYC 7742, 2Ã— 64 cores, 2.25 GHz
c) 512 (16Ã— 32) GB DDR4, 3200 MHz

3. 12 login nodes for the initial access and coding.
a) 2Ã— NVIDIA Quadro RTX8000 GPU
b) 2Ã— AMD EPYC 7742, 2Ã— 64 cores, 2.25 GHz
c) 1024 (16Ã— 64) GB DDR4, 3200 MHz

54

Register at: judoor.fz-juelich.de/login

Click here!

55

Enable the Multi-Factor Authentication (MFA)

Click here!

56

Sign the User Agreement

57

Manage SSH-Keys to access the GPUs
Use/create SSH keys: ssh-keygen -t ed25519 -C "your_email"

Use SSH keys:

Generate SSH keys:

58

copy

Add the ssh key here:
cat id_ed25519.pub or
ssh-keygen -t ed25519 -C "your_email goes here"

59

Wait up to 15 min until you can use the key

60

Connected to the Juelich Login Node
adam$ ssh -i ~/ssh/id_ed25519 dziedzic4@jureca.fz-juelich.de
(dziedzic4@jureca.fz-juelich.de) JSC TOTP Verification code: Use your multi-factor authenticator like FreeOTP or
Google Authenticator
********************************************************************************
**** Welcome to JURECA (Juelich Research on Exascale Cluster Architectures) ****
********************************************************************************
* Information about the system, latest changes, user documentation and FAQs: *
* -> https://go.fzj.de/JURECA -> https://go.fzj.de/jureca-known-issues
*
* Slurm job reports:
*
* -> https://go.fzj.de/llview-jureca
*
********************************************************************************
* Status information also at https://go.fzj.de/status-jureca-dc
*
********************************************************************************
2025-03-31T15:54+0200
Information - Do not constrain GPU devices with Slurm
Due to a bug in Slurm the GPU devices will not be constrained by cgroups until
it is properly fixed.
-------------------------------------------------------------------------------********************************************************************************

61

Use Visual Studio Code for the Development

62

Configure SSH Hosts

63

Choose the default config file

64

Add the connection to the Juelich cluster

65

Then you can connect directly to the cluster

66

Open new terminal

67

Request the allocation with the command
Then, request the allocation of the GPUs with the command:
salloc -p dc-gpu -t 20 -N 1 -A training2557

68

Use the allocated node
Then, use the allocated node with the command:
srun --pty bash -i

69

Then you are ready to use the GPUs

70

Questions?

