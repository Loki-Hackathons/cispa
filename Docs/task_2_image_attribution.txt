European Championship in Trustworthy AI

Dr. Adam Dziedzic and Dr. Franziska Boenisch

Image Attribution

Task: Image Attribution

Goal

The goal of this task is to determine whether a given image was generated by one of four
known generative models VAR-16d, RAR-XL, Taming-cIN, and Stable Diffusion (SD) or
whether it originates from an unknown model or is a natural image (outlier). Your objective is
to attribute each image to one of the four known sources or classify it as an outlier.

Challenge
Participants receive a limited number of labeled examples from the four known models. The
main challenge is to design a method that generalizes well to unseen samples. A critical
aspect of this task is out-of-distribution (OOD) detection: you must identify images that do
not belong to any of the four known model families and classify them as an outlier.

Evaluation Metric
Your model will be evaluated using True Positive Rate and False Positive Rate,
macro-averaged across the four in-distribution classes (RAR, Taming, VAR, SD).

â— True Positive Rate (TPR) â€“ Measures how effectively the model recognizes images
from each known model.
â— False Positive Rate (FPR) â€“ Is capped at 1%, enforcing robustness against
misclassifying outliers as being obtained from known models.
Each class receives a weighted score:

ğ¹ğ‘–ğ‘›ğ‘ğ‘™ ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ 
ğ¶ = ğ‘‡ğ‘ƒğ‘…
ğ‘/ (1 + (ğ‘Š * ğ¹ğ‘ƒğ‘…
ğ‘))

The final leaderboard score is the macro-average of over the four known
ğ¹ğ‘–ğ‘›ğ‘ğ‘™ ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ğ¶
classes.

Dataset

A single archive (dataset.zip) is provided containing three predefined splits:

1. Training (labeled, in-distribution only) - 1000 images in total
a. RAR: 250
b. Taming: 250
c. VAR: 250
d. SD: 250
2. Validation (labeled, includes OOD samples) - 500 images in total
a. RAR: 100
b. Taming: 100
c. VAR: 100
d. SD: 100
e. OOD: 100

3. Test (unlabeled) - 9000 images total
Purpose: Used for final leaderboard scoring. Ground-truth labels are hidden.
Total dataset size: 10,500 images.

â— (also available at HuggingFace or on juelich login nodes at
/p/project1/training2557/common/image-attribution)
What You Have Access To

â— Labeled training data for the four known models.
â— Labeled validation data (including OOD samples).
â— Unlabeled test data for final evaluation.
â— Optional scripts for generating additional synthetic data from RAR, and VAR.
â— A starter-kit script (task_template.py in the repository).
Your Task
Assign exactly one label to each image in the test dataset. Possible labels: VAR, RAR,
Taming, SD, outlier
Your model must output a single class label for each image.

Additional Resources
A starter kit is provided that includes:

â— Code to unzip and load dataset splits.
â— A dummy ResNet-18 model example.
â— A function that generates random predictions for the test set and saves them to
submission.csv.
Use the starter code as a template and replace the random predictions with your modelâ€™s
outputs.

You may optionally use provided generator scripts to synthesize more labeled samples:

â— RAR.py â†’ RAR-XL model
â— VAR.py â†’ VAR-16d model
Each script can create a virtual environment, download pre-trained weights, and save
generated images into an outputs/ folder. You can modify class IDs or use command-line
options to control which ImageNet categories are generated.

Submission Format
Prepare a CSV file named submission.csv with this structure:

image_name,label

img1.png,RAR

img2.png,outlier

img3.png,Taming

Requirements:

â— Every test image must appear exactly once.
â— Each image filename must end with .png.
â— Labels must be one of: RAR, Taming, VAR, SD, outlier.

â— Spelling must exactly match the allowed labels.
â— File size must not exceed 10 MB
Scoring

â— Submissions are evaluated against hidden ground-truth labels.
â— The leaderboard score is based on the Final Score described above.
â— This metric encourages high detection rates while enforcing very low false positives.
How to Get Started

1. Install dependencies - pip install torch torchvision pillow requests
2. Place the dataset - Put Dataset.zip in the same directory as the starter script.
3. Run the starter kit - python task_template.py
a. This will unzip the dataset, load splits, initialize a dummy model, produce
random predictions, and save them to submission.csv.
4. Check your submission file. Ensure submission.csv contains exactly one line per test
image.
Leaderboard and Evaluation

â— Immediate public leaderboard uses 30% of test data; remaining 70% used for final
evaluation after the deadline.
â— The scoreboard displays your best submission per team.
Example Command for Submission

import pandas as pd

import requests

df = pd.DataFrame({

"image_name": image_names,

"label": predictions,

})

df.to_csv("submission.csv", index=None)

response = requests.post(

"http://34.122.51.94:9000",

files={"file": open("submission.csv", "rb")},

headers={"token": "YOUR_TEAM_TOKEN"}

)

print(response.json())

Leaderboard
After evaluation, your results can be found in the leaderboard:

â— You can access the leaderboard for this task at
http://34.122.51.94:80/leaderboard_page. This will help you to compare your
solutions with other teams and see where you stand.


â— The leaderboard shows the best result per team only. As output to your request, you
will get back the score for your current submission. If it is lower than the score saved
in the leaderboard, the score will not be updated.
References

1. Chao Wang, Kejiang Chen, Zijin Yang, Yaofei Wang, and Weiming Zhang. â€œAEDR:
Training-free ai-generated image attribution via autoencoder double-reconstruction.â€
https://arxiv.org/abs/2507.18988
2. Zhenting Wang, Vikash Sehwag, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, and
Shiqing Ma. â€œHow to trace latent generative model generated images without artificial
watermark?â€ ICML 2024 https://openreview.net/forum?id=TwZ2sY6eJj
3. â€œVisual Autoregressive Modeling: Scalable Image Generation via Next-Scale
Predictionâ€ NeurIPS 2024 https://openreview.net/forum?id=gojL67CfS8
4. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen â€œRandomized
Autoregressive Visual Generationâ€ 2024 https://yucornetto.github.io/projects/rar.html
5. Patrick Esser, Robin Rombach, and Bjorn Ommer. â€œTaming transformers for
high-resolution image synthesis.â€ http://bit.ly/49avnSJ
Check, for example, the latent tracer Reference [2]:



